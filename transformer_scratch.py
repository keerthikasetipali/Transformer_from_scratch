# -*- coding: utf-8 -*-
"""Transformer_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pgot7QwDwFNi0PcZGZ-ZtBs3eGFThW5Z
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math

# Tokenization class
class customtokenize:
    def __init__(self, vocab):
        self.vocab = vocab
        self.stoi = {s: i for i, s in enumerate(self.vocab)}
        self.itos = {i: s for s, i in self.stoi.items()}

    def tokenize(self, text):
        tokens = text.split()
        token_id = [self.stoi[token] if token in self.stoi else self.stoi['<unk>'] for token in tokens]
        return token_id

# Re-tokenize the text ensuring all tokens are in vocabulary
#def safe_tokenize(tokenizer, text, vocab_len):
  #tokens = text.split()
  #token_ids = [tokenizer.stoi[token] if token in tokenizer.stoi else tokenizer.stoi['<unk>'] for token in tokens]
  #token_ids = [id if id < vocab_len else tokenizer.stoi['<unk>'] for id in token_ids]
  #return token_ids

# Embedding class
class TransformerEmbeddings(nn.Module):
    def __init__(self, vocab_size, emb_size):
        super(TransformerEmbeddings, self).__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)

    def forward(self,token_id):
        return self.embedding(token_id)

# Positional encoding function
def positionalencoding(seq_length, emb_size):
    seq_length = int(seq_length)
    emb_size = int(emb_size)
    PE = np.zeros((seq_length, emb_size))
    for pos in range(seq_length):
        for i in range(emb_size):
            if i % 2 == 0:
                PE[pos, i] = np.sin(pos / (10000 ** (2 * i / emb_size)))
            else:
                PE[pos, i] = np.cos(pos / (10000 ** (2 * (i - 1) / emb_size)))
    return PE

# Self-attention class
class self_attention(nn.Module):
    def __init__(self, emb_size):
        super(self_attention, self).__init__()
        self.emb_size = emb_size
        self.query = nn.Linear(1,1,bias=False)
        self.key = nn.Linear(1, 1,bias=False)
        self.value = nn.Linear(1, 1,bias=False)
        self.final_out = nn.Linear(emb_size, emb_size)

    def forward(self, input, mask=None):
        q = self.query(enc_input)
        k = self.key(enc_input)
        v = self.value(enc_input)
        att_scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.emb_size)
        if mask is not None:
            att_scores = att_scores.masked_fill(mask == 0, float("-1e9"))
        attention_weights = F.softmax(att_scores, dim=-1)
        out = torch.matmul(attention_weights, v)
        final_output = self.final_out(out)
        return final_output

# Layer normalization class
class layernorm(nn.Module):
    def __init__(self, emb_size, eps=1e-6):
        super(layernorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(emb_size))
        self.beta = nn.Parameter(torch.zeros(emb_size))
        self.eps = eps

    def forward(self, enc_input):
        mean = enc_input.mean(-1, keepdim=True)
        std = enc_input.std(-1, keepdim=True)
        return self.gamma * (enc_input - mean) / (std + self.eps) + self.beta

# Feedforward class
class feedforward(nn.Module):
    def __init__(self, emb_size, hidden_size):
        super(feedforward, self).__init__()
        self.linear1 = nn.Linear(emb_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, emb_size)

    def forward(self, input):
        return self.linear2(F.relu(self.linear1(enc_input)))

# Encoder layer class
class encoderlayer(nn.Module):
    def __init__(self, emb_size, hidden_size, dropout=0.1):
        super(encoderlayer, self).__init__()
        self.self_atten = self_attention(emb_size)
        self.norm1 = layernorm(emb_size)
        self.norm2 = layernorm(emb_size)
        self.feed_forward = feedforward(emb_size, hidden_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, enc_input, mask=None):
        # Self-attention sublayer
        attention_out = self.self_atten(enc_input, mask)
        x = enc_input + self.dropout(attention_out)  # Residual connection
        x = self.norm1(x)  # Normalize the combined output

        # Feed-forward sublayer
        final_output = self.feed_forward(x)
        x = x + self.dropout(final_output)  # Residual connection
        x = self.norm2(x)  # Normalize the combined output

        return x

class Encoder(nn.Module):
    def __init__(self, num_layers, emb_size, num_heads,hidden_size, vocab_size, seq_length, dropout=0.1):
        super(Encoder, self).__init__()
        self.emb_size = emb_size
        self.embedding = TransformerEmbeddings(vocab_size, emb_size)
        self.positional_encoding = torch.from_numpy(positionalencoding(seq_length, emb_size)).float().unsqueeze(0)
        self.layers = nn.ModuleList([encoderlayer(emb_size, hidden_size, dropout) for _ in range(num_layers)])
        self.dropout = nn.Dropout(dropout)

    def forward(self, enc_input, mask=None):
        embeddings = self.embedding(enc_input)
        seq_length = embeddings.shape(1)
        positional_encoding = self.positional_encoding[:, :seq_length, :]
        x = embeddings + positional_encoding.to(embeddings.device)
        x = self.dropout(x)

        for layer in self.layers:
            x = layer(x, mask)

        return x

class decoderlayer(nn.Module):
  def __init__(self,emb_size,hidden_size,dropout=0.1):
    super(decoderlayer,self).__init__()
    self.masked_atten=self_attention(emb_size)
    self.cross_atten=self_attention(emb_size)
    self.norm1=layernorm(emb_size)
    self.norm2=layernorm(emb_size)
    self.norm3=layernorm(emb_size)
    self.feed_forward=feedforward(emb_size, hidden_size)
    self.dropout=nn.Dropout(dropout)
  def forward(self,dec_inputs,selfmask,encoder_mask):
    # Self-attention sublayer
        atten_out = self.masked_atten(dec_inputs, selfmask)
        x = dec_inputs + self.dropout(atten_out)
        x = self.norm1(x)

        # Encoder-decoder attention sublayer
        encoder_atten_out = self.cross_atten(x, enc_output, encoder_mask)
        x = x + self.dropout(encoder_atten_out)
        x = self.norm2(x)

        # Feed-forward sublayer
        ff_out = self.feed_forward(x)
        x = x + self.dropout(ff_out)
        x = self.norm3(x)

        return x

class Decoder(nn.Module):
    def __init__(self, num_layers, emb_size, num_heads, hidden_size, vocab_len, seq_length, dropout=0.1):
        super(Decoder, self).__init__()
        #num_layers=int(num_layers)
        #seq_length=int(seq_length)
        self.num_layers = num_layers
        self.embedding = nn.Embedding(vocab_len, emb_size)
        self.positional_encoding = nn.Parameter(torch.from_numpy(positionalencoding(seq_length, emb_size)).float().unsqueeze(0))
        self.layers = nn.ModuleList([decoderlayer(emb_size, hidden_size, dropout) for _ in range(num_layers)])
        self.linear = nn.Linear(emb_size, vocab_len)
        self.dropout=nn.Dropout(dropout)


    def forward(self, dec_inputs, enc_output, dec_mask=None, enc_dec_mask=None):
        dec_outputs = self.embedding(dec_inputs)
        dec_outputs = self.positional_encoding+dec_outputs

        for layer in self.layers:
            dec_outputs = layer(dec_outputs, enc_output, dec_mask, enc_dec_mask)

        logits = self.linear(dec_outputs)
        return logits

class Transformer(nn.Module):
  def __init__(self,num_layers,emb_size,hidden_size,vocab_len,seq_length,deropout=0.1):
    super(Transformer,self).__init__()
    assert emb_size % num_heads == 0, "emb_size must be divisible by num_heads"
    self.encoder=Encoder(num_layers,emb_size,hidden_size,vocab_len,seq_length,dropout)
    self.decoder=Decoder(num_layers,emb_size,hidden_size,vocab_len,seq_length,dropout)

  def forward(self,enc_input,dec_inputs,enc_mask=None,dec_mask=None,enc_dec_mask=None):
    enc_output=self.encoder(enc_input,enc_mask)
    dec_output=self.decoder(dec_inputs,enc_output,dec_mask,enc_dec_mask)
    return dec_output

# Function to read the text file
def read_text_file(file_path):
    with open(file_path, 'r') as file:
        text = file.read()
    return text

# File path to the text file
file_path = '/content/sam.txt'

# Read text from the file
text = read_text_file(file_path)
#text="certain copyright implications you Project Gutenberg is proud to cooperate with The World Library in the presentation of The Complete Works of William Shakespearefor your reading for education and entertainment.  HOWEVER, THIS IS NEITHER SHAREWARE NOR PUBLIC DOMAIN. . .AND UNDER THE LIBRARYOF THE FUTURE CONDITIONS OF THIS PRESENTATION. . .NO CHARGES MAYBE MADE FOR *ANY* ACCESS TO THIS MATERIAL.  YOU ARE ENCOURAGED!!TO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE ALLOWED"
# Vocabulary
max_vocab_size = 1000
vocab = ["<pad>", "<unk>"] + list(set(text.split()[:max_vocab_size]))
tokenizer = customtokenize(vocab)
token_ids = tokenizer.tokenize(text)
vocab_len = len(vocab)
valid_token_ids = [id if id < vocab_len else tokenizer.stoi['<unk>'] for id in token_ids]
input_tensor = torch.tensor(valid_token_ids).unsqueeze(0)  # Add batch dimension
 # Convert token IDs to a tensor
#input_tensor = torch.tensor(token_ids, dtype=torch.long)

# Convert token IDs to tensor
#token_ids = safe_tokenize(tokenizer, text, vocab_len)

#input_tensor = torch.tensor(token_ids).unsqueeze(0)  # Add batch dimension

input_tensor.shape

#emb_size = 546
#vocab_len = len(vocab)
emb_size=vocab_len
embedding_model = TransformerEmbeddings(vocab_len, emb_size)
embeddings = embedding_model(input_tensor)

# Positional encodings
seq_length = embeddings.shape[1]
positional_encodings = positionalencoding(seq_length, emb_size)
pos_enc = torch.from_numpy(positional_encodings).float().unsqueeze(0)  # Add batch dimension

pos_enc.shape

embeddings.shape

# Add embeddings and positional encodings
enc_input = embeddings + pos_enc

enc_input.dtype

# Encoder layer
hidden_size = 16
encoder_layer = encoderlayer(emb_size, hidden_size)

# Forward pass
enc_output = encoder_layer(enc_input)

vocab_len

enc_output.dtype

print(enc_input[:1])

print(enc_output[:1])

dec_inputs=embeddings+pos_enc

enc_input==dec_inputs

enc_output

# Define model parameters
num_layers = 6
vocab_size = len(vocab)
seq_length = 100
dropout = 0.1
num_heads=5
#emb_size=546

vocab_size

emb_size

num_heads

seq_length

#model building
model = Transformer(num_layers, emb_size, hidden_size, vocab_size, seq_length, dropout)

enc_input

dec_inputs

#enc_input=enc_input.unsqueeze(0).int()
#dec_inputs=dec_inputs.unsqueeze(0).int()

enc_input = enc_input.long()
dec_inputs = dec_inputs.long()

enc_input

dec_inputs

enc_input.shape

dec_inputs.shape

enc_input

dec_inputs

print(token_ids)

print("Embedding values:", embeddings)

output=model(enc_input,dec_inputs)

